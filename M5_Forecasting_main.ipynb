{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import psutil\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reduce memory usage of pandas dataframes by casting columns to the most memory efficient data type\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain lag features\n",
    "\n",
    "def getLagFeatures(features, NUM_LAG_DAYS):\n",
    "    for i in range(1,NUM_LAG_DAYS+1):\n",
    "        curr = 'lag'+str(i)\n",
    "        features[curr] = features.groupby('id')['sales'].shift(i)\n",
    "    features = features.dropna()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain rolling window features (mean, sum, max, min, std)\n",
    "\n",
    "def getAggregates(features, NUM_LAG_DAYS):\n",
    "    num_weeks = NUM_LAG_DAYS // 7\n",
    "    for i in range(1,num_weeks+1):\n",
    "        start = (i-1)*7+1\n",
    "        end = start + 7\n",
    "        cols = ['lag'+str(j) for j in range(start,end)]\n",
    "        same_day_cols = ['lag'+str(k*7) for k in range(1,num_weeks+1)]\n",
    "        features['lag_week'+str(i)+'_sum'] = features[cols].sum(axis=1)\n",
    "        features['lag_week'+str(i)+'_mean'] = features[cols].mean(axis=1)\n",
    "        features['lag_week'+str(i)+'_min'] = features[cols].min(axis=1)\n",
    "        features['lag_week'+str(i)+'_max'] = features[cols].max(axis=1)\n",
    "        features['lag_week'+str(i)+'_std'] = features[cols].std(axis=1)\n",
    "        features['same_day_mean'] = features[same_day_cols].mean(axis=1)\n",
    "        features['same_day_std'] = features[same_day_cols].std(axis=1)\n",
    "        if i == 1:\n",
    "            continue\n",
    "        else:\n",
    "            features['lag_week_diff'+str(i-1)] = features['lag_week'+str(i)+'_mean']-features['lag_week'+str(i-1)+'_mean']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain price features\n",
    "\n",
    "def getPriceFeatures(prices,id_map):\n",
    "    prices['last_week_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].shift(1)\n",
    "    prices['max_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "    prices['min_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "    prices['mean_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "    prices['price_diff'] = prices['sell_price'] - prices['last_week_sell_price']\n",
    "    prices['price_std'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "    prices['price_norm'] = prices['sell_price']/prices['max_sell_price']\n",
    "    prices['price_unique'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "    prices['item_unique'] = prices.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "    prices = id_map.merge(prices, on=['store_id','item_id'],how='left')\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain date features\n",
    "\n",
    "def getDateFeatures(calendar):\n",
    "    calendar = calendar.fillna('None')\n",
    "    calendar['is_weekend'] = calendar['wday'].apply(lambda x: 1 if (x==1 or x==2) else 0)\n",
    "    calendar['is_weekend'] = calendar['is_weekend'].astype('category')\n",
    "    calendar['event_type_1'] = calendar['event_type_1'].astype('category')\n",
    "    calendar['event_name_1'] = calendar['event_name_1'].astype('category')\n",
    "    calendar['event_type_2'] = calendar['event_type_2'].astype('category')\n",
    "    calendar['event_name_2'] = calendar['event_name_2'].astype('category')\n",
    "    calendar['snap_CA'] = calendar['snap_CA'].astype('uint8')\n",
    "    calendar['snap_TX'] = calendar['snap_TX'].astype('uint8')\n",
    "    calendar['snap_WI'] = calendar['snap_WI'].astype('uint8')\n",
    "    return calendar[['d','wm_yr_wk','wday','month','event_name_1','event_name_2','event_type_1','event_type_2','snap_CA','snap_TX','snap_WI']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to set up a base dataframe for merging purposes\n",
    "\n",
    "def getBaseFeatures(ids, days, calendar, prices):\n",
    "    base_days = np.asarray([[d]*len(ids) for d in days]).flatten()\n",
    "    base_ids = ids*len(days)\n",
    "    base = pd.DataFrame({'id': base_ids,'d':base_days})\n",
    "    base = base.merge(calendar,on=['d'],how='left')\n",
    "    base = base.merge(prices, on=['wm_yr_wk','id'],how='left')\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get lag and aggregate features on-the-fly during prediction phase\n",
    "\n",
    "def getLagFeaturesRecursive(df, d, NUM_LAG_DAYS):\n",
    "    prev = 'sales'\n",
    "    for i in range(1,NUM_LAG_DAYS+1):\n",
    "        curr = 'lag'+str(i)\n",
    "        df.loc[df.d==d,curr]=df.loc[df.d==d-1,prev].values\n",
    "        prev = curr\n",
    "    num_weeks = NUM_LAG_DAYS//7\n",
    "    for i in range(1,num_weeks+1):\n",
    "        start = (i-1)*7+1\n",
    "        end = start+7\n",
    "        cols = ['lag'+str(j) for j in range(start,end)]\n",
    "        same_day_cols = ['lag'+str(k*7) for k in range(1,num_weeks+1)]\n",
    "        df.loc[df.d==d,'lag_week'+str(i)+'_sum'] = df[cols].sum(axis=1)\n",
    "        df.loc[df.d==d,'lag_week'+str(i)+'_mean'] = df[cols].mean(axis=1)\n",
    "        df.loc[df.d==d,'lag_week'+str(i)+'_min'] = df[cols].min(axis=1)\n",
    "        df.loc[df.d==d,'lag_week'+str(i)+'_max'] = df[cols].max(axis=1)\n",
    "        df.loc[df.d==d,'lag_week'+str(i)+'_std'] = df[cols].std(axis=1)\n",
    "        df.loc[df.d==d,'same_day_mean'] = df[same_day_cols].mean(axis=1)\n",
    "        df.loc[df.d==d,'same_day_std'] = df[same_day_cols].std(axis=1)\n",
    "        if i == 1: \n",
    "            continue\n",
    "        else:\n",
    "            df.loc[df.d==d,'lag_week_diff'+str(i-1)] = df.loc[df.d==d,'lag_week'+str(i)+'_mean'].values - df.loc[df.d==d,'lag_week'+str(i-1)+'_mean'].values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):\n",
    "    return np.sqrt(mse(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train an lightgbm model\n",
    "\n",
    "def train_full(store_features,store,desc, NUM_LAG_DAYS):\n",
    "    model_file = store+'_'+desc+'.txt'\n",
    "    train = store_features[(store_features.d<=1863) & (store_features.d>=1069+NUM_LAG_DAYS)].dropna()\n",
    "    val = store_features[(store_features.d > 1863) & (store_features.d < 1914)].dropna()\n",
    "    test = store_features[store_features.d >= 1914]\n",
    "    del store_features\n",
    "    drop_cols = ['sales','id','d','wm_yr_wk','original_id','store_id','item_id']\n",
    "    num_weeks = NUM_LAG_DAYS // 7\n",
    "    for j in range(1,num_weeks+1):\n",
    "        drop_cols += ['lag'+str(j) for j in range(j*7-6,j*7)]\n",
    "    categorical_cols = ['event_name_1','event_name_2','event_type_1','event_type_2']\n",
    "    x_train = train.drop(drop_cols,axis=1)\n",
    "    y_train = train.sales.values\n",
    "    x_val = val.drop(drop_cols,axis=1)\n",
    "    y_val = val.sales.values    \n",
    "\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',         \n",
    "        'objective': 'regression',       \n",
    "        'metric': ['rmse'],             \n",
    "        'subsample': 0.8,                \n",
    "        'subsample_freq': 1,\n",
    "        'learning_rate': 0.03,           \n",
    "        'num_leaves': 2**9-1,            \n",
    "        'min_data_in_leaf': 2**8-1,      \n",
    "        'feature_fraction': 0.8,\n",
    "        'n_estimators': 5000,            \n",
    "        'early_stopping_rounds': 30,     \n",
    "        'verbose': -1,\n",
    "        'max_bin':2**9-1\n",
    "            } \n",
    "    train_set = lgb.Dataset(x_train, y_train)\n",
    "    val_set = lgb.Dataset(x_val, y_val)\n",
    "    lgb_model = lgb.train(lgb_params, train_set, num_boost_round = 2000, valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "    lgb_model.save_model(model_file)\n",
    "    val_pred_lgb = lgb_model.predict(x_val, num_iteration=lgb_model.best_iteration)\n",
    "    val_score_lgb = rmse(val_pred_lgb, y_val)\n",
    "    \n",
    "    print(f'final val rmse score: {val_score_lgb}')\n",
    "    del x_train, y_train\n",
    "    return lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict sales for a specified day\n",
    "\n",
    "def predict(features, lgb_model, d,NUM_LAG_DAYS):\n",
    "    X = features[features.d == d]\n",
    "    drop_cols = ['sales','id','d','wm_yr_wk','original_id','store_id','item_id']\n",
    "    categorical_cols = ['event_name_1','event_name_2','event_type_1','event_type_2']\n",
    "    num_weeks = NUM_LAG_DAYS // 7\n",
    "    for j in range(1,num_weeks+1):\n",
    "        drop_cols += ['lag'+str(j) for j in range(j*7-6,j*7)]\n",
    "    X = X.drop(drop_cols,axis=1)\n",
    "    Y = lgb_model.predict(X,num_iteration=lgb_model.best_iteration)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to perform the 3 phases: feature engineering, model training and prediction\n",
    "\n",
    "def run_store(store,sales,prices,calendar,id_map,desc,NUM_LAG_DAYS):\n",
    "    store_features = sales[sales.store_id == store]\n",
    "    store_features = getLagFeatures(store_features,NUM_LAG_DAYS)\n",
    "    store_features = getAggregates(store_features,NUM_LAG_DAYS)\n",
    "    store_features = store_features.drop(['item_id','store_id'],axis=1)\n",
    "    ids = list(store_features.id.unique())\n",
    "    days = list(calendar.d.values)\n",
    "    base = getBaseFeatures(ids,days,getDateFeatures(calendar),getPriceFeatures(prices,id_map))\n",
    "    store_features = base.merge(store_features,on=['id','d'],how='left')\n",
    "    del base\n",
    "    lgb_model = train_full(store_features,store,desc,NUM_LAG_DAYS)\n",
    "    for i in tqdm(range(1914,1970)):\n",
    "        store_features = getLagFeaturesRecursive(store_features, i, NUM_LAG_DAYS)\n",
    "        store_features.loc[store_features.d == i,'sales'] = predict(store_features,lgb_model,i,NUM_LAG_DAYS)\n",
    "\n",
    "    store_pred = store_features[store_features.d > 1913]\n",
    "    store_pred = store_pred[['id','d','sales']]\n",
    "    \n",
    "    today = datetime.date.today().strftime(\"%d%m\")\n",
    "    store_pred.to_csv(store+'_'+desc+'.csv')\n",
    "    del store_features\n",
    "    del store_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full(stores,sales,prices,calendar,id_map,model_file,NUM_LAG_DAYS):\n",
    "    for store in tqdm(stores):\n",
    "        run_store(store,sales,prices,calendar,id_map,model_file,NUM_LAG_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"2104_morePF_newParams_lag28only_2014onwards_increaseNumLeaves\"\n",
    "NUM_LAG_DAYS = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sales\n",
    "sales = pd.read_pickle('sales_grid.pkl')\n",
    "sales = sales[sales.d >= 1069] # 2014 onwards\n",
    "\n",
    "# intialize calendar\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "calendar.d = calendar.d.apply(lambda d: int(d.split('_')[1])).apply(pd.to_numeric,downcast='unsigned')\n",
    "calendar = calendar[calendar.d >= 1069] #2014 onwards\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "\n",
    "# initialize prices\n",
    "prices = pd.read_csv('sell_prices.csv')\n",
    "prices = reduce_mem_usage(prices)\n",
    "\n",
    "# initialize id map\n",
    "id_map = pd.read_csv('id_map.csv')\n",
    "stores = sales.store_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls on run_store for every store\n",
    "run_full(stores,sales,prices,calendar,id_map,desc,NUM_LAG_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate kaggle submission file \n",
    "\n",
    "def submit(desc,stores,id_map):\n",
    "    sub_cols = ['id']+['F'+str(i) for i in range(1,29)]\n",
    "    sub_val = pd.DataFrame(columns=sub_cols)\n",
    "    sub_eval = pd.DataFrame(columns=sub_cols)\n",
    "    for store in stores:\n",
    "        filename = store+'_'+desc+'.csv'\n",
    "        store_df = pd.read_csv(filename)[['id','d','sales']]\n",
    "        store_df = store_df.pivot_table(index=['id'],columns='d').reset_index()\n",
    "        store_df.columns = ['id']+[str(i) for i in range(1914,1970)]\n",
    "        val_cols = ['id']+[str(i) for i in range(1914,1942)]\n",
    "        eval_cols = ['id']+[str(i) for i in range(1942,1970)]\n",
    "        val_df = store_df[val_cols]\n",
    "        eval_df = store_df[eval_cols]\n",
    "        \n",
    "        val_df = id_map.drop(['item_id','store_id'],axis=1).merge(val_df,on='id')\n",
    "        val_df['original_id'] = val_df['original_id'].apply(lambda x: '_'.join(x.split('_')[:-1])+'_validation')\n",
    "        val_df = val_df.drop(['id'],axis=1)\n",
    "        val_df.columns = sub_cols\n",
    "        sub_val = sub_val.append(val_df)\n",
    "        \n",
    "        eval_df = id_map.drop(['item_id','store_id'],axis=1).merge(eval_df,on='id')\n",
    "        eval_df['original_id'] = eval_df['original_id'].apply(lambda x: '_'.join(x.split('_')[:-1])+'_evaluation')\n",
    "        eval_df = eval_df.drop(['id'],axis=1)\n",
    "        eval_df.columns = sub_cols\n",
    "        sub_eval = sub_eval.append(eval_df)\n",
    "    sub_val = sub_val.append(sub_eval)\n",
    "    return sub_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_val= submit(desc,stores,id_map)\n",
    "sub_val.reset_index().drop(['index'],axis=1).to_csv('submission_'+desc+'.csv',index=False)\n",
    "sub_cols = ['F'+str(i) for i in range(1,29)]\n",
    "sub_val[sub_cols] = sub_val[sub_cols].round(0)\n",
    "sub_val.to_csv('submission_rounded_'+desc+'.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
