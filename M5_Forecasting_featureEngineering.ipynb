{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper to reduce memory of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reduce memory usage of pandas dataframes by casting columns to the most memory efficient data type\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lag features\n",
    "\n",
    "This function generates lag features. The function takes in NUM_LAG_DAYS as a parameter, which specifies how many days into the past we are looking at. \n",
    "\n",
    "Pre-condition: NUM_LAG_DAYS must be a multiple of 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain lag features\n",
    "\n",
    "def getLagFeatures(features, NUM_LAG_DAYS):\n",
    "    for i in range(1,NUM_LAG_DAYS+1):\n",
    "        curr = 'lag'+str(i)\n",
    "        features[curr] = features.groupby('id')['sales'].shift(i)\n",
    "    features = features.dropna()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Rolling aggregates\n",
    "\n",
    "This function generates rolling window features. The window is defined to be 7 days, so we generate the maximum, minimum, mean, sum and standard deviation for k 7-day periods where k = number of weeks into the past we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain rolling window features (mean, sum, max, min, std)\n",
    "\n",
    "def getAggregates(features, NUM_LAG_DAYS):\n",
    "    num_weeks = NUM_LAG_DAYS // 7\n",
    "    for i in range(1,num_weeks+1):\n",
    "        start = (i-1)*7+1\n",
    "        end = start + 7\n",
    "        cols = ['lag'+str(j) for j in range(start,end)]\n",
    "        same_day_cols = ['lag'+str(k*7) for k in range(1,num_weeks+1)]\n",
    "        features['lag_week'+str(i)+'_sum'] = features[cols].sum(axis=1)\n",
    "        features['lag_week'+str(i)+'_mean'] = features[cols].mean(axis=1)\n",
    "        features['lag_week'+str(i)+'_min'] = features[cols].min(axis=1)\n",
    "        features['lag_week'+str(i)+'_max'] = features[cols].max(axis=1)\n",
    "        features['lag_week'+str(i)+'_std'] = features[cols].std(axis=1)\n",
    "        features['same_day_mean'] = features[same_day_cols].mean(axis=1)\n",
    "        features['same_day_std'] = features[same_day_cols].std(axis=1)\n",
    "        if i == 1:\n",
    "            continue\n",
    "        else:\n",
    "            features['lag_week_diff'+str(i-1)] = features['lag_week'+str(i)+'_mean']-features['lag_week'+str(i-1)+'_mean']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price features\n",
    "\n",
    "From the price dataset, we can generate several features related to the product price, such as maximum/minimum sell price and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain price features\n",
    "\n",
    "def getPriceFeatures(prices,id_map):\n",
    "    prices['last_week_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].shift(1)\n",
    "    prices['max_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "    prices['min_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "    prices['mean_sell_price'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "    prices['price_diff'] = prices['sell_price'] - prices['last_week_sell_price']\n",
    "    prices['price_std'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "    prices['price_norm'] = prices['sell_price']/prices['max_sell_price']\n",
    "    prices['price_unique'] = prices.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "    prices['item_unique'] = prices.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "    prices = id_map.merge(prices, on=['store_id','item_id'],how='left')\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date features\n",
    "\n",
    "For date features, they are mostly given in the calendar dataset. The only preprocessing step to be done is to convert some of the columns to category type so as to further reduce memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain date features\n",
    "\n",
    "def getDateFeatures(calendar):\n",
    "    calendar = calendar.fillna('None')\n",
    "    calendar['is_weekend'] = calendar['wday'].apply(lambda x: 1 if (x==1 or x==2) else 0)\n",
    "    calendar['is_weekend'] = calendar['is_weekend'].astype('category')\n",
    "    calendar['event_type_1'] = calendar['event_type_1'].astype('category')\n",
    "    calendar['event_name_1'] = calendar['event_name_1'].astype('category')\n",
    "    calendar['event_type_2'] = calendar['event_type_2'].astype('category')\n",
    "    calendar['event_name_2'] = calendar['event_name_2'].astype('category')\n",
    "    calendar['snap_CA'] = calendar['snap_CA'].astype('uint8')\n",
    "    calendar['snap_TX'] = calendar['snap_TX'].astype('uint8')\n",
    "    calendar['snap_WI'] = calendar['snap_WI'].astype('uint8')\n",
    "    return calendar[['d','wm_yr_wk','wday','month','event_name_1','event_name_2','event_type_1','event_type_2','snap_CA','snap_TX','snap_WI']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to set up a base dataframe for merging purposes\n",
    "\n",
    "def getBaseFeatures(ids, days, calendar, prices):\n",
    "    base_days = np.asarray([[d]*len(ids) for d in days]).flatten()\n",
    "    base_ids = ids*len(days)\n",
    "    base = pd.DataFrame({'id': base_ids,'d':base_days})\n",
    "    base = base.merge(calendar,on=['d'],how='left')\n",
    "    base = base.merge(prices, on=['wm_yr_wk','id'],how='left')\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.06 Mb (44.2% reduction)\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "# initialize sales\n",
    "sales = pd.read_pickle('sales_grid.pkl')\n",
    "sales = sales[sales.d >= 1069] # 2014 onwards\n",
    "\n",
    "# intialize calendar\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "calendar.d = calendar.d.apply(lambda d: int(d.split('_')[1])).apply(pd.to_numeric,downcast='unsigned')\n",
    "calendar = calendar[calendar.d >= 1069] #2014 onwards\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "\n",
    "# initialize prices\n",
    "prices = pd.read_csv('sell_prices.csv')\n",
    "prices = reduce_mem_usage(prices)\n",
    "\n",
    "# initialize id map\n",
    "id_map = pd.read_csv('id_map.csv')\n",
    "stores = sales.store_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features at store level\n",
    "\n",
    "We generate and save features at store level to work within the kernel memory li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n",
      "Mem. usage decreased to 458.48 Mb (65.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "NUM_LAG_DAYS = 28\n",
    "\n",
    "for store in stores:\n",
    "    store_features = sales[sales.store_id == store]\n",
    "    store_features = getLagFeatures(store_features,NUM_LAG_DAYS)\n",
    "    store_features = getAggregates(store_features,NUM_LAG_DAYS)\n",
    "    store_features = store_features.drop(['item_id','store_id'],axis=1)\n",
    "    ids = list(store_features.id.unique())\n",
    "    days = list(calendar.d.values)\n",
    "    base = getBaseFeatures(ids,days,getDateFeatures(calendar),getPriceFeatures(prices,id_map))\n",
    "    store_features = base.merge(store_features,on=['id','d'],how='left')\n",
    "    store_features = reduce_mem_usage(store_features)\n",
    "    store_features.to_pickle(store+\"_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
